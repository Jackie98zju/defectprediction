\begin{table*}
	\centering
	\renewcommand\tabcolsep{1.0pt}
	\begin{tabular}{lllllllllllllllllll}
		\hline
		$filename$ & \multicolumn{3}{c}{nsga2} & \multicolumn{3}{c}{revised-nsga2} & \multicolumn{3}{c}{learning-to-rank} & \multicolumn{3}{c}{lasso} & \multicolumn{3}{c}{ridge} & \multicolumn{3}{c}{lr}\\
		$target$ & $fpa$ & $mse$ & $nnz$ & $fpa$ & $mse$ & $nnz$ & $fpa$ & $mse$ & $nnz$ & $fpa$ & $mse$ & $nnz$ & $fpa$ & $mse$ & $nnz$ & $fpa$ & $mse$ & $nnz$\\
		\hline
		$ant$-1.3-1.4 & 0.6035 & 942.6 & 21 & 0.5981 & 0.3 & 1 & 0.573 & 60402.4 & 21 & 0.5781 & 0.3 & 4 & 0.6018 & 0.4 & 21 \\
		$ant$-1.4-1.5 & 0.7022 & 47122.8 & 21 & 0.762 & 0.1 & 2 & 0.7117 & 95296.2 & 21 & 0.7695 & 0.1 & 5 & 0.7591 & 0.1 & 21 \\
		$ant$-1.5-1.6 & 0.7108 & 38881.9 & 21 & 0.8122 & 1.4 & 2 & 0.728 & 520141.4 & 21 & 0.817 & 1.3 & 8 & 0.8077 & 1.3 & 21 \\
		$ant$-1.6-1.7 & 0.8046 & 6046.7 & 21 & 0.8274 & 0.8 & 1 & 0.8196 & 1076029.4 & 21 & 0.8283 & 0.7 & 4 & 0.8218 & 0.7 & 21 \\
		$camel$-1.0-1.2 & 0.5723 & 1383.4 & 21 & 0.6749 & 3.2 & 2 & 0.5912 & 753864.8 & 21 & 0.5008 & 4.3 & 1 & 0.6098 & 4.2 & 21 \\
		$camel$-1.2-1.4 & 0.7085 & 1742.1 & 21 & 0.7628 & 1.7 & 1 & 0.765 & 31747.3 & 21 & 0.7593 & 1.7 & 5 & 0.6949 & 2.7 & 21 \\
		$camel$-1.4-1.6 & 0.7171 & 1296.4 & 21 & 0.7391 & 3.4 & 1 & 0.7461 & 115387.6 & 21 & 0.7544 & 3.0 & 6 & 0.7489 & 2.9 & 21 \\
		$ivy$-1.1-1.4 & 0.6005 & 89629.8 & 21 & 0.7697 & 22.1 & 1 & 0.7199 & 1310496.3 & 21 & 0.7683 & 125.4 & 9 & 0.7732 & 147.8 & 21 \\
		$ivy$-1.4-2.0 & 0.7031 & 29508.8 & 21 & 0.8066 & 0.2 & 2 & 0.7856 & 113684.5 & 21 & 0.7481 & 0.2 & 4 & 0.506 & 0.3 & 21 \\
		$jedit$-3.2-4.0 & 0.8415 & 62402.9 & 21 & 0.8398 & 4.0 & 1 & 0.8447 & 345493.9 & 21 & 0.8382 & 6.8 & 15 & 0.8353 & 7.6 & 21 \\
		$jedit$-4.0-4.1 & 0.7971 & 29106.2 & 21 & 0.8216 & 2.0 & 1 & 0.8231 & 193787.9 & 21 & 0.7947 & 1.5 & 15 & 0.804 & 1.6 & 21 \\
		$jedit$-4.1-4.2 & 0.8669 & 24430.8 & 21 & 0.864 & 1.2 & 1 & 0.8686 & 1349167.9 & 21 & 0.8805 & 2.6 & 21 & 0.8826 & 2.5 & 21 \\
		$jedit$-4.2-4.3 & 0.7166 & 79331.4 & 21 & 0.6651 & 0.5 & 1 & 0.6885 & 294236.2 & 21 & 0.6506 & 0.7 & 21 & 0.6479 & 0.7 & 21 \\
		$log4j$-1.0-1.1 & 0.794 & 4412.1 & 21 & 0.7568 & 1.2 & 1 & 0.7984 & 60544.3 & 21 & 0.8008 & 1.0 & 12 & 0.8033 & 1.0 & 21 \\
		$log4j$-1.1-1.2 & 0.5484 & 28144.8 & 21 & 0.5607 & 4.4 & 2 & 0.5572 & 104261.8 & 21 & 0.5622 & 8.8 & 8 & 0.5656 & 11.3 & 21 \\
		$lucene$-2.0-2.2 & 0.6908 & 6048.0 & 21 & 0.7162 & 8.0 & 1 & 0.7103 & 306802.1 & 21 & 0.7193 & 6.8 & 9 & 0.7191 & 6.9 & 21 \\
		$lucene$-2.2-2.4 & 0.6415 & 11889.6 & 21 & 0.6951 & 4.5 & 1 & 0.6881 & 3126255.4 & 21 & 0.6942 & 5.9 & 6 & 0.6557 & 14.1 & 21 \\
		$poi$-1.5-2.0 & 0.6995 & 48334.9 & 21 & 0.7009 & 16.1 & 2 & 0.7255 & 185382.0 & 21 & 0.7086 & 4.4 & 5 & 0.6933 & 5.5 & 21 \\
		$poi$-2.0-2.5 & 0.5134 & 36392.5 & 21 & 0.4543 & 28.9 & 2 & 0.5024 & 126469.5 & 21 & 0.5013 & 3.3 & 1 & 0.4727 & 3.2 & 21 \\
		$poi$-2.5-3.0 & 0.6476 & 44174.8 & 21 & 0.6869 & 4.9 & 2 & 0.6603 & 128931.3 & 21 & 0.7015 & 2.7 & 2 & 0.6891 & 2.4 & 21 \\
		$synapse$-1.0-1.1 & 0.6426 & 5009.7 & 21 & 0.7105 & 0.8 & 1 & 0.6391 & 67376.2 & 21 & 0.7387 & 0.9 & 6 & 0.7236 & 0.9 & 21 \\
		$synapse$-1.1-1.2 & 0.6616 & 3461.2 & 21 & 0.6534 & 1.1 & 2 & 0.6231 & 2319190.7 & 21 & 0.681 & 0.9 & 6 & 0.6749 & 0.9 & 21 \\
		$velocity$-1.4-1.5 & 0.6095 & 17839.0 & 21 & 0.6213 & 5.0 & 3 & 0.6476 & 336200.1 & 21 & 0.6202 & 4.6 & 20 & 0.6212 & 4.8 & 21 \\
		$velocity$-1.5-1.6 & 0.7437 & 6491.3 & 21 & 0.7125 & 3.1 & 2 & 0.7602 & 283754.2 & 21 & 0.737 & 2.6 & 9 & 0.7465 & 2.8 & 21 \\
		$xalan$-2.4-2.5 & 0.6334 & 32639.7 & 21 & 0.6333 & 1.0 & 1 & 0.6475 & 1126950.9 & 21 & 0.6333 & 0.9 & 4 & 0.6468 & 0.9 & 21 \\
		$xalan$-2.5-2.6 & 0.6646 & 14914.1 & 21 & 0.6615 & 23.5 & 3 & 0.66 & 753957.2 & 21 & 0.6508 & 0.8 & 19 & 0.6529 & 0.8 & 21 \\
		$xalan$-2.6-2.7 & 0.5673 & 67928.1 & 21 & 0.5699 & 1.5 & 1 & 0.5689 & 722538.0 & 21 & 0.5588 & 0.9 & 6 & 0.5538 & 0.9 & 21 \\
		$xerces$-$init$-1.2 & 0.7025 & 21576.8 & 21 & 0.6046 & 38.5 & 3 & 0.6355 & 594894.2 & 21 & 0.5071 & 1.3 & 5 & 0.5699 & 1.5 & 21 \\
		$xerces$-1.2-1.3 & 0.6687 & 4519.9 & 21 & 0.6165 & 4.5 & 5 & 0.717 & 98963.4 & 21 & 0.7137 & 3.5 & 5 & 0.6986 & 3.2 & 21 \\
		$xerces$-1.3-1.4 & 0.675 & 3190.2 & 21 & 0.74 & 29.4 & 2 & 0.7142 & 70067.8 & 21 & 0.7524 & 26.0 & 4 & 0.6767 & 27.9 & 21 \\
		\hline
		$average$ & 0.6816 & 25626.4 & 21.0 & 0.7013 & 7.2 & 1.7 & 0.6974 & 555742.5 & 21.0 & 0.699 & 7.5 & 8.2 & 0.6886 & 8.7 & 21.0 \\
		\hline
	\end{tabular}
	\caption{Accuracies of prediction models}
	\label{tab:accuracy}
\end{table*}

